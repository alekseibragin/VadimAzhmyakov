{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6fa6766",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Created on Thu Dec 16 15:50:11 2021\n",
    "\n",
    "@author: Vadim\n",
    "\"\"\"\n",
    "import os\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "import tf_agents.trajectories.time_step as ts\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.policies.policy_saver import PolicySaver\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "\n",
    "class PPOTrainer:\n",
    "    \n",
    "    def __init__(self, ppo_agents, train_env, eval_env, size=(96, 96),\n",
    "                 normalize=True, num_frames=1, num_channels=3,\n",
    "                 use_tensorboard=True, add_to_video=True,\n",
    "                 use_separate_agents=False, use_self_play=False,\n",
    "                 num_agents=2, use_lstm=False, experiment_name=\"\",\n",
    "                 collect_steps_per_episode=1000, total_epochs=1000,\n",
    "                 total_steps=1e6, eval_steps_per_episode=1000,\n",
    "                 eval_interval=100, num_eval_episodes=5, epsilon=0.0,\n",
    "                 save_interval=500, log_interval=1):\n",
    "\n",
    "        self.train_env = train_env\n",
    "        self.eval_env = eval_env  \n",
    "\n",
    "        self.size = size\n",
    "        self.H, self.W = self.size[0], self.size[1]  \n",
    "        self.normalize = normalize\n",
    "        self.num_frames = num_frames\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.use_separate_agents = use_separate_agents  \n",
    "        self.use_self_play = use_self_play  \n",
    "        self.use_lstm = use_lstm  \n",
    "        self.num_agents = num_agents\n",
    "\n",
    "        self.max_buffer_size = collect_steps_per_episode \n",
    "        self.collect_steps_per_episode = collect_steps_per_episode  \n",
    "        self.epochs = total_epochs  \n",
    "        self.total_steps = total_steps  \n",
    "        self.global_step = 0  \n",
    "        self.epsilon = epsilon \n",
    "\n",
    "        print(\"Total steps: {}\".format(self.total_steps))\n",
    "\n",
    "        # Create N different PPO agents\n",
    "        if use_separate_agents and self.num_agents > 1:\n",
    "            self.agents = ppo_agents  \n",
    "            for agent in self.agents:\n",
    "                agent.initialize()  \n",
    "            self.actor_nets = [self.agents[i]._actor_net \\\n",
    "                               for i in range(self.num_agents)]\n",
    "            self.value_nets = [self.agents[i]._value_net \\\n",
    "                               for i in range(self.num_agents)]\n",
    "            self.eval_policies = [self.agents[i].policy \\\n",
    "                                  for i in range(self.num_agents)]\n",
    "            self.collect_policies = [self.agents[i].collect_policy \\\n",
    "                                     for i in range(self.num_agents)]\n",
    "            self.replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "                self.agents[0].collect_data_spec,\n",
    "                batch_size=self.train_env.batch_size,\n",
    "                max_length=self.max_buffer_size)  # Create shared replay buffer\n",
    "\n",
    "        else:\n",
    "            self.agent = ppo_agents\n",
    "            self.actor_net = self.agent._actor_net\n",
    "            self.value_net = self.agent._value_net\n",
    "            self.eval_policy = self.agent.policy\n",
    "            self.collect_policy = self.agent.collect_policy\n",
    "            self.replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "                self.agent.collect_data_spec,\n",
    "                batch_size=self.train_env.batch_size,\n",
    "                max_length=self.max_buffer_size)\n",
    "\n",
    "        if self.num_agents > 1:  \n",
    "            self.observation_wrappers = \\\n",
    "                            [ObservationWrapper(size=self.size, normalize=self.normalize,\n",
    "                                                num_channels=self.num_channels,\n",
    "                                                num_frames=self.num_frames)\n",
    "                             for i in range(self.num_agents)]\n",
    "\n",
    "        else:  # Single observation wrapper for single car\n",
    "            self.observation_wrapper = ObservationWrapper(size=self.size,\n",
    "                                                          normalize=self.normalize,\n",
    "                                                          num_channels=self.num_channels,\n",
    "                                                          num_frames=self.num_frames)\n",
    "\n",
    "        # Evaluation\n",
    "        self.num_eval_episodes = num_eval_episodes  \n",
    "\n",
    "        if self.use_separate_agents:\n",
    "            self.eval_returns = [[] for i in range(self.num_agents)]\n",
    "\n",
    "        else:\n",
    "            self.eval_returns = []\n",
    "\n",
    "        self.eval_interval = eval_interval  \n",
    "        self.max_eval_episode_steps = eval_steps_per_episode  \n",
    "        self.time_ext = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "        self.log_interval = log_interval\n",
    "        self.video_train = []\n",
    "        self.video_eval = []\n",
    "        self.add_to_video = add_to_video\n",
    "        self.FPS = 50  \n",
    "        self.policy_save_dir = os.path.join(os.path.split(__file__)[0], \"models\",\n",
    "                                            experiment_name.format(self.time_ext))\n",
    "        self.save_interval = save_interval\n",
    "        if not os.path.exists(self.policy_save_dir):\n",
    "            print(\"Directory {} does not exist;\"\n",
    "                  \" creating it now\".format(self.policy_save_dir))\n",
    "            os.makedirs(self.policy_save_dir, exist_ok=True)\n",
    "\n",
    "        if self.use_separate_agents:\n",
    "            self.train_savers = [PolicySaver(self.collect_policies[i],\n",
    "                                             batch_size=None) for i in\n",
    "                                 range(self.num_agents)]\n",
    "            self.eval_savers = [PolicySaver(self.eval_policies[i],\n",
    "                                            batch_size=None) for i in\n",
    "                                range(self.num_agents)]\n",
    "\n",
    "        else:\n",
    "            self.train_saver = PolicySaver(self.collect_policy, batch_size=None)\n",
    "            self.eval_saver = PolicySaver(self.eval_policy, batch_size=None)\n",
    "\n",
    "        self.log_dir = os.path.join(os.path.split(__file__)[0], \"logging\",\n",
    "                                    experiment_name.format(self.time_ext))\n",
    "        self.tb_file_writer = tf.summary.create_file_writer(self.log_dir)\n",
    "        if not os.path.exists(self.log_dir):\n",
    "            os.makedirs(self.log_dir, exist_ok=True)\n",
    "        self.use_tensorboard = use_tensorboard  \n",
    "        self.size = size\n",
    "        self.H, self.W = self.size\n",
    "        self.stacked_channels = self.num_channels * self.num_frames\n",
    "        if self.use_tensorboard:\n",
    "            self.tb_gif_train = np.zeros((self.collect_steps_per_episode,\n",
    "                                         self.num_agents, self.H, self.W,\n",
    "                                          self.stacked_channels))\n",
    "            self.tb_gif_eval = np.zeros((self.max_eval_episode_steps,\n",
    "                                        self.num_agents, self.H, self.W,\n",
    "                                         self.stacked_channels))\n",
    "        # Devices\n",
    "        local_device_protos = device_lib.list_local_devices()\n",
    "        num_gpus = len([x.name for x in local_device_protos if\n",
    "                        x.device_type == 'GPU'])\n",
    "        self.use_gpu = num_gpus > 0\n",
    "\n",
    "    def is_last(self, mode='train'):\n",
    "        \n",
    "        if mode == 'train':\n",
    "            step_types = self.train_env.current_time_step().step_type.numpy()\n",
    "        elif mode == 'eval':\n",
    "            step_types = self.eval_env.current_time_step().step_type.numpy()\n",
    "\n",
    "\n",
    "        is_last = bool(min(np.count_nonzero(step_types == 2), 1))\n",
    "        return is_last\n",
    "\n",
    "    def get_agent_timesteps(self, time_step, step=0,\n",
    "                            only_ego_car=False, ego_car_index=0, max_steps=1000):\n",
    "        \n",
    "        discount = time_step.discount\n",
    "        if len(discount.numpy().shape) > 1 or discount.numpy().shape[0] > 1:\n",
    "            discount = discount[0]\n",
    "        discount = tf.convert_to_tensor(discount, dtype=tf.float32,\n",
    "                                        name='discount')\n",
    "\n",
    "        \n",
    "        if step == 0:  \n",
    "            step_type = 0\n",
    "        elif step == max_steps-1:  \n",
    "            step_type = 2\n",
    "        else:  # Middle time step\n",
    "            step_type = 1\n",
    "        step_type = tf.convert_to_tensor([step_type], dtype=tf.int32,\n",
    "                                         name='step_type')\n",
    "\n",
    "        # Extract rewards for all agents\n",
    "        try:\n",
    "            R = [tf.convert_to_tensor(time_step.reward[:, car_id],\n",
    "                                      dtype=tf.float32, name='reward') \\\n",
    "                 for car_id in range(self.num_agents)]\n",
    "        except:\n",
    "            R = [tf.convert_to_tensor(time_step.reward,\n",
    "                                      dtype=tf.float32, name='reward') \\\n",
    "                 for _ in range(self.num_agents)]\n",
    "\n",
    "        if only_ego_car:\n",
    "            processed_observation = \\\n",
    "                self._process_observations(time_step, only_ego_car=only_ego_car,\n",
    "                                           ego_car_index=ego_car_index)\n",
    "            return ts.TimeStep(step_type, R[ego_car_index], discount,\n",
    "                                   tf.convert_to_tensor(processed_observation,\n",
    "                                   dtype=tf.float32, name='observations'))\n",
    "\n",
    "        else:\n",
    "            processed_observations = \\\n",
    "                self._process_observations(time_step, only_ego_car=only_ego_car,\n",
    "                                           ego_car_index=ego_car_index)\n",
    "            return [ts.TimeStep(step_type, R[car_id], discount,\n",
    "                    tf.convert_to_tensor(processed_observations[car_id],\n",
    "                                         dtype=tf.float32, name='observations'))\n",
    "                    for car_id in range(self.num_agents)]\n",
    "\n",
    "    def _process_observations(self, time_step, only_ego_car=False,\n",
    "                              ego_car_index=0):\n",
    "\n",
    "        if only_ego_car:\n",
    "            input_observation = time_step.observation[:, ego_car_index]\n",
    "\n",
    "            if self.num_agents > 1:  \n",
    "                wrapper = self.observation_wrappers[ego_car_index]\n",
    "            else:\n",
    "                wrapper = self.observation_wrapper\n",
    "\n",
    "            processed_observation = wrapper.get_obs_and_step(input_observation)\n",
    "\n",
    "            return tf.convert_to_tensor(processed_observation, dtype=tf.float32,\n",
    "                                        name='observations')\n",
    "\n",
    "        else:\n",
    "            input_observations = [time_step.observation[:, car_id] for\n",
    "                                  car_id in range(self.num_agents)]\n",
    "            if self.num_agents > 1:  \n",
    "                processed_observations = \\\n",
    "                    [wrapper.get_obs_and_step(input_observation)\n",
    "                     for wrapper, input_observation in\n",
    "                     zip(self.observation_wrappers, input_observations)]\n",
    "\n",
    "            else:  # Single car\n",
    "                processed_observations = \\\n",
    "                    [self.observation_wrapper.get_obs_and_step(\n",
    "                        input_observations[0])]\n",
    "\n",
    "            return [tf.convert_to_tensor(processed_observations[car_id],\n",
    "                                    dtype=tf.float32, name='observations')\n",
    "                    for car_id in range(self.num_agents)]\n",
    "\n",
    "    def collect_step(self, step=0, ego_car_index=0, use_greedy=False,\n",
    "                     add_to_video=False):\n",
    "        \n",
    "        time_step = self.train_env.current_time_step()\n",
    "\n",
    "        actions = []\n",
    "\n",
    "        agent_timesteps = self.get_agent_timesteps(time_step, step=step,\n",
    "                                                   only_ego_car=False,\n",
    "                                                   max_steps=self.max_eval_episode_steps-1)\n",
    "\n",
    "        if self.use_separate_agents:\n",
    "            ego_agent_policy = self.collect_policies[ego_car_index]\n",
    "        else:\n",
    "            ego_agent_policy = self.collect_policy\n",
    "\n",
    "\n",
    "        NUM_AGENTS = 1 \n",
    "        for car_id in range(NUM_AGENTS):\n",
    "\n",
    "            if car_id == ego_car_index:\n",
    "                ego_agent_ts = agent_timesteps[car_id]\n",
    "                ego_action_step = ego_agent_policy.action(ego_agent_ts)\n",
    "                if use_greedy:\n",
    "                    actions.append(ego_action_step.info['loc'])  \n",
    "                else:\n",
    "                    actions.append(ego_action_step.action)  \n",
    "\n",
    "                if self.add_to_video:\n",
    "                    rendered_state = time_step.observation[:, car_id].numpy()\n",
    "                    if self.stacked_channels > 3: \n",
    "                        rendered_state = rendered_state[:, :, :, :3] \n",
    "                    self.video_train.append(rendered_state)\n",
    "\n",
    "            elif self.use_separate_agents:\n",
    "                other_agent_ts = agent_timesteps[car_id]\n",
    "                action_step = self.eval_policies[car_id].action(other_agent_ts)\n",
    "                actions.append(action_step.action)\n",
    "\n",
    "            elif self.use_self_play:\n",
    "                other_agent_ts = agent_timesteps[car_id]\n",
    "                action_step = self.eval_policy.action(other_agent_ts)\n",
    "                actions.append(action_step.action)\n",
    "\n",
    "        if self.use_tensorboard:\n",
    "            processed_observations = self._process_observations(time_step,\n",
    "                                                               only_ego_car=False)\n",
    "            self.tb_gif_train[step] = tf.convert_to_tensor(processed_observations)\n",
    "\n",
    "        action_tensor = tf.convert_to_tensor([tf.stack(tuple(actions), axis=1)])\n",
    "\n",
    "        next_time_step = self.train_env.step(action_tensor)\n",
    "        ego_agent_next_ts = self.get_agent_timesteps(next_time_step, step=step+1,\n",
    "                                                     ego_car_index=ego_car_index,\n",
    "                                                     only_ego_car=True,\n",
    "                                                     max_steps=self.collect_steps_per_episode-1)\n",
    "\n",
    "        traj = trajectory.from_transition(ego_agent_ts, ego_action_step,\n",
    "                                          ego_agent_next_ts)\n",
    "        self.replay_buffer.add_batch(traj)\n",
    "\n",
    "        if add_to_video:\n",
    "            rendered_state = time_step.observation[:, ego_car_index].numpy()\n",
    "            if self.num_frames > 1:  \n",
    "                rendered_state = rendered_state[:, :, :, :3]  # First frame\n",
    "            self.video_train.append(rendered_state)\n",
    "\n",
    "        return float(ego_agent_ts.reward)\n",
    "\n",
    "    def collect_episode(self, epoch=0, ego_car_index=0, add_to_video=False):\n",
    "        \n",
    "        episode_reward = 0  \n",
    "        step = 0  \n",
    "\n",
    "        self.train_env.reset()\n",
    "\n",
    "        use_greedy = float(np.random.binomial(n=1, p=self.epsilon))\n",
    "\n",
    "        while step < self.collect_steps_per_episode and \\\n",
    "                not self.is_last(mode='train'):\n",
    "            episode_reward += self.collect_step(add_to_video=add_to_video,\n",
    "                                                step=step, use_greedy=use_greedy,\n",
    "                                                ego_car_index=ego_car_index)\n",
    "            step += 1\n",
    "\n",
    "        self.global_step += step\n",
    "\n",
    "        if self.use_tensorboard:\n",
    "            with self.tb_file_writer.as_default():\n",
    "                tf.summary.scalar(\"Average Training Reward\", float(episode_reward),\n",
    "                                  step=self.global_step)\n",
    "                frames = self.tb_gif_train\n",
    "                video_summary(\"train/grid\", frames, fps=self.FPS,\n",
    "                              step=self.global_step, channels=self.num_channels)\n",
    "\n",
    "            self.tb_gif_train = np.zeros((self.collect_steps_per_episode,\n",
    "                                         self.num_agents, self.H, self.W,\n",
    "                                          self.stacked_channels))\n",
    "\n",
    "    def compute_average_reward(self, ego_car_index=0):\n",
    "        \n",
    "        total_return = 0.0  \n",
    "\n",
    "        for e in range(self.num_eval_episodes):\n",
    "            time_step = self.eval_env.reset()\n",
    "\n",
    "            # Initialize step counter and episode_return\n",
    "            i = 0\n",
    "            episode_return = 0.0\n",
    "            while i < self.max_eval_episode_steps and \\\n",
    "                    not self.is_last(mode='eval'):\n",
    "                actions = []\n",
    "\n",
    "                agent_timesteps = self.get_agent_timesteps(time_step, step=i)\n",
    "                for car_id in range(self.num_agents):\n",
    "\n",
    "                    if car_id == ego_car_index:  \n",
    "                        ego_agent_ts = agent_timesteps[car_id]\n",
    "                        rendered_state = ego_agent_ts.observation.numpy()\n",
    "                        if self.num_frames > 1:  \n",
    "                            rendered_state = rendered_state[..., :3] \n",
    "                        self.video_eval.append(rendered_state)\n",
    "\n",
    "                        if self.use_separate_agents:  \n",
    "                            ego_action_step = self.eval_policies[car_id].action(ego_agent_ts)\n",
    "                            actions.append(ego_action_step.action)   \n",
    "\n",
    "                        elif self.use_self_play:  \n",
    "                            ego_action_step = self.eval_policy.action(ego_agent_ts)\n",
    "                            actions.append(ego_action_step.action) \n",
    "\n",
    "                    elif self.use_separate_agents:\n",
    "                        other_agent_ts = agent_timesteps[car_id]\n",
    "                        action_step = self.eval_policies[car_id].action(other_agent_ts)\n",
    "                        actions.append(action_step.action)\n",
    "\n",
    "                   \n",
    "                    elif self.use_self_play:\n",
    "                        other_agent_ts = agent_timesteps[car_id]\n",
    "                        action_step = self.eval_policy.action(other_agent_ts)\n",
    "                        actions.append(action_step.action)\n",
    "\n",
    "                action_tensor = tf.convert_to_tensor([tf.stack(tuple(actions),\n",
    "                                                               axis=1)])\n",
    "\n",
    "                time_step = self.eval_env.step(action_tensor)\n",
    "\n",
    "                if self.use_tensorboard:\n",
    "                    processed_observations = self._process_observations(time_step,\n",
    "                                                                        only_ego_car=False)\n",
    "                    self.tb_gif_eval[i] = tf.convert_to_tensor(processed_observations)\n",
    "\n",
    "                episode_return += ego_agent_ts.reward  \n",
    "                if i % 250 == 0:\n",
    "                    action = ego_action_step.action.numpy()\n",
    "                    print(\"Action: {}, \"\n",
    "                          \"Reward: {}\".format(action, episode_return))\n",
    "                i += 1\n",
    "\n",
    "            print(\"Steps in episode: {}\".format(i))\n",
    "            total_return += episode_return\n",
    "        avg_return = total_return / self.num_eval_episodes\n",
    "\n",
    "        if self.use_tensorboard:\n",
    "            with self.tb_file_writer.as_default():\n",
    "                video_summary(\"eval/grid\".format(car_id), self.tb_gif_eval,\n",
    "                              fps=self.FPS, step=self.global_step,\n",
    "                              channels=self.num_channels)\n",
    "\n",
    "            self.tb_gif_eval = np.zeros((self.max_eval_episode_steps,\n",
    "                                        self.num_agents, self.H, self.W,\n",
    "                                         self.stacked_channels))\n",
    "\n",
    "        print(\"Average return: {}\".format(avg_return))\n",
    "\n",
    "\n",
    "        if self.use_separate_agents:  \n",
    "            self.eval_returns[ego_car_index].append(avg_return)\n",
    "        else:  \n",
    "            self.eval_returns.append(avg_return)\n",
    "\n",
    "        return avg_return\n",
    "\n",
    "    def collect_step_lstm(self, step=0, ego_car_index=0, add_to_video=False,\n",
    "                          policy_states=None):\n",
    "        \n",
    "        time_step = self.train_env.current_time_step()\n",
    "\n",
    "        # Create empty list of actions and next policy states\n",
    "        actions = []\n",
    "        next_policy_states = {}\n",
    "\n",
    "        agent_timesteps = self.get_agent_timesteps(time_step, step=step,\n",
    "                                                   only_ego_car=False)\n",
    "\n",
    "        if self.use_separate_agents:\n",
    "            ego_agent_policy = self.collect_policies[ego_car_index]\n",
    "        else:\n",
    "            ego_agent_policy = self.collect_policy\n",
    "\n",
    "        for car_id in range(self.num_agents):\n",
    "            if car_id == ego_car_index:\n",
    "                ego_agent_ts = agent_timesteps[car_id]\n",
    "                if self.use_separate_agents:\n",
    "                    ego_policy_step = ego_agent_policy.action(\n",
    "                        ego_agent_ts, policy_states[car_id])\n",
    "                else:\n",
    "                    ego_policy_step = self.collect_policy.action(ego_agent_ts,\n",
    "                                                                 policy_states[car_id])\n",
    "                if use_greedy:\n",
    "                    actions.append(ego_action_step.info['loc']) \n",
    "                else:\n",
    "                    actions.append(ego_action_step.action)  \n",
    "                policy_state = ego_policy_step.state\n",
    "\n",
    "                if self.add_to_video:\n",
    "                    rendered_state = time_step.observation[:, car_id].numpy()\n",
    "                    if self.num_frames > 1:  \n",
    "                        rendered_state = rendered_state[..., :3] \n",
    "                    self.video_train.append(rendered_state)\n",
    "\n",
    "            elif self.use_separate_agents:\n",
    "                other_agent_ts = agent_timesteps[car_id]\n",
    "                policy_step = self.eval_policies[car_id].action(other_agent_ts, policy_states[car_id])\n",
    "                policy_state = policy_step.state\n",
    "                actions.append(policy_step.action)  \n",
    "\n",
    "            elif self.use_self_play:\n",
    "                other_agent_ts = agent_timesteps[car_id]\n",
    "                policy_step = self.eval_policy.action(other_agent_ts, policy_states[car_id])\n",
    "                policy_state = policy_step.state\n",
    "                actions.append(policy_step.action)\n",
    "\n",
    "            next_policy_states[car_id] = policy_state  \n",
    "\n",
    "        if self.use_tensorboard:\n",
    "            processed_observations = self._process_observations(time_step,\n",
    "                                                                only_ego_car=False)\n",
    "            self.tb_gif_train[step] = tf.convert_to_tensor(processed_observations)\n",
    "\n",
    "        action_tensor = tf.convert_to_tensor([tf.stack(tuple(actions), axis=1)])\n",
    "\n",
    "        next_time_step = self.train_env.step(action_tensor)\n",
    "        ego_agent_next_ts = self.get_agent_timesteps(next_time_step,\n",
    "                                                     step=step + 1,\n",
    "                                                     ego_car_index=ego_car_index,\n",
    "                                                     only_ego_car=True)\n",
    "\n",
    "        traj = trajectory.from_transition(ego_agent_ts, ego_policy_step,\n",
    "                                          ego_agent_next_ts)\n",
    "        self.replay_buffer.add_batch(traj)\n",
    "\n",
    "        if add_to_video:\n",
    "            rendered_state = time_step.observation[:, ego_car_index].numpy()\n",
    "            if self.num_frames > 1:\n",
    "                rendered_state = rendered_state[:, :, :, 3]  \n",
    "            self.video_train.append(rendered_state)\n",
    "\n",
    "        return next_policy_states, float(ego_agent_ts.reward)\n",
    "\n",
    "    def reset_policy_states(self, ego_car_index=0, mode='train'):\n",
    "        \n",
    "        if mode == 'train':\n",
    "            if self.use_separate_agents:\n",
    "                policy_states = {car_id: self.eval_policies[car_id].get_initial_state(\n",
    "                    self.train_env.batch_size) for car_id in range(self.num_agents)}\n",
    "                policy_states[ego_car_index] = self.collect_policies[\n",
    "                    ego_car_index].get_initial_state(self.train_env.batch_size)\n",
    "            else:\n",
    "                policy_states = {car_id: self.eval_policy.get_initial_state(self.train_env.batch_size)\n",
    "                                 for car_id in range(self.num_agents)}\n",
    "                policy_states[ego_car_index] = self.collect_policy.get_initial_state(\n",
    "                    self.train_env.batch_size)\n",
    "\n",
    "        elif mode == 'eval':\n",
    "            if self.use_separate_agents:\n",
    "                policy_states = {\n",
    "                    car_id: self.eval_policies[car_id].get_initial_state(\n",
    "                        self.eval_env.batch_size) for car_id in\n",
    "                    range(self.num_agents)}\n",
    "            else:\n",
    "                policy_states = {car_id: self.eval_policy.get_initial_state(\n",
    "                    self.eval_env.batch_size) for car_id in\n",
    "                    range(self.num_agents)}\n",
    "\n",
    "        return policy_states\n",
    "\n",
    "    def collect_episode_lstm(self, epoch=0, ego_car_index=0, add_to_video=False):\n",
    "        \n",
    "        policy_states = self.reset_policy_states(ego_car_index=ego_car_index)\n",
    "\n",
    "        episode_reward = 0  \n",
    "        step = 0\n",
    "\n",
    "        self.train_env.reset()\n",
    "\n",
    "        use_greedy = float(np.random.binomial(n=1, p=self.epsilon))\n",
    "\n",
    "        while step < self.collect_steps_per_episode and \\\n",
    "                not self.is_last(mode='train'):\n",
    "            if step % 1000 == 0:\n",
    "                print(\"Step number: {}\".format(step))\n",
    "            policy_states, ego_reward = self.collect_step_lstm(add_to_video=add_to_video,\n",
    "                                                               step=step, use_greedy=use_greedy,\n",
    "                                                               ego_car_index=ego_car_index,\n",
    "                                                               policy_states=policy_states)\n",
    "            episode_reward += ego_reward\n",
    "            step += 1\n",
    "\n",
    "        self.global_step += step\n",
    "\n",
    "        if self.use_tensorboard:\n",
    "            with self.tb_file_writer.as_default():\n",
    "                tf.summary.scalar(\"Average Training Reward\", float(episode_reward),\n",
    "                                  step=self.global_step)\n",
    "                frames = self.tb_gif_train\n",
    "                video_summary(\"train/grid\", frames, fps=self.FPS,\n",
    "                              step=self.global_step,\n",
    "                              channels=self.num_channels)\n",
    "\n",
    "            self.tb_gif_train = np.zeros((self.collect_steps_per_episode,\n",
    "                                          self.num_agents, self.H, self.W,\n",
    "                                          self.stacked_channels))\n",
    "\n",
    "    def compute_average_reward_lstm(self, ego_car_index=0):\n",
    "        \n",
    "        total_return = 0.0\n",
    "\n",
    "        for _ in range(self.num_eval_episodes):\n",
    "            time_step = self.eval_env.reset()\n",
    "\n",
    "            i = 0\n",
    "            episode_return = 0.0\n",
    "\n",
    "            policy_states = self.reset_policy_states(ego_car_index=ego_car_index,\n",
    "                                                     mode='eval')\n",
    "\n",
    "            while i < self.max_eval_episode_steps and \\\n",
    "                    not self.is_last(mode='eval'):\n",
    "\n",
    "                actions = []\n",
    "\n",
    "                agent_timesteps = self.get_agent_timesteps(time_step, step=i)\n",
    "                NUM_AGENTS = 1\n",
    "                for car_id in range(NUM_AGENTS):\n",
    "\n",
    "                    if car_id == ego_car_index:\n",
    "                        ego_agent_ts = agent_timesteps[car_id]\n",
    "                        rendered_state = ego_agent_ts.observation.numpy()\n",
    "                        if self.num_frames > 1:\n",
    "                            rendered_state = rendered_state[..., :3]  \n",
    "                        self.video_eval.append(rendered_state)\n",
    "\n",
    "                        if self.use_separate_agents:  \n",
    "                            ego_policy_step = self.eval_policies[car_id].action(ego_agent_ts,\n",
    "                                                                                policy_states[car_id])\n",
    "                            actions.append(ego_policy_step.action)  \n",
    "\n",
    "                        elif self.use_self_play:  \n",
    "                            ego_policy_step = self.eval_policy.action(ego_agent_ts,\n",
    "                                                                      policy_states[car_id])\n",
    "                            actions.append(ego_policy_step.action)  \n",
    "\n",
    "                        policy_state = ego_policy_step.state\n",
    "\n",
    "                    elif self.use_separate_agents:\n",
    "                        other_agent_ts = agent_timesteps[car_id]\n",
    "                        policy_step = self.eval_policies[car_id].action(other_agent_ts,\n",
    "                                                                        policy_states[car_id])\n",
    "                        actions.append(policy_step.action)\n",
    "                        policy_state = policy_step.state\n",
    "\n",
    "                    elif self.use_self_play:\n",
    "                        other_agent_ts = agent_timesteps[car_id]\n",
    "                        policy_step = self.eval_policy.action(other_agent_ts,\n",
    "                                                              policy_states[car_id])\n",
    "                        actions.append(policy_step.action)\n",
    "                        policy_state = policy_step.state\n",
    "\n",
    "                    policy_states[car_id] = policy_state\n",
    "\n",
    "                action_tensor = tf.convert_to_tensor([tf.stack(tuple(actions), axis=1)])\n",
    "\n",
    "                time_step = self.eval_env.step(action_tensor)\n",
    "\n",
    "                if self.use_tensorboard:\n",
    "                    processed_observations = self._process_observations(time_step,\n",
    "                                                                        only_ego_car=False)\n",
    "                    self.tb_gif_eval[i] = tf.convert_to_tensor(processed_observations)\n",
    "\n",
    "                episode_return += ego_agent_ts.reward \n",
    "                if i % 250 == 0:\n",
    "                    action = ego_policy_step.action.numpy()\n",
    "                    print(\"Action: {}, \"\n",
    "                          \"Reward: {}\".format(action, episode_return))\n",
    "                    print(\"POLICY STATES: {}\".format(\n",
    "                        [np.sum(policy_states[i]) for i\n",
    "                         in range(self.num_agents)]))\n",
    "                i += 1\n",
    "            print(\"Steps in episode: {}\".format(i))\n",
    "            total_return += episode_return\n",
    "        avg_return = total_return / self.num_eval_episodes\n",
    "\n",
    "        \n",
    "        print(\"Average return: {}\".format(avg_return))\n",
    "\n",
    "        if self.use_separate_agents:  \n",
    "            self.eval_returns[ego_car_index].append(avg_return)\n",
    "        else: \n",
    "            self.eval_returns.append(avg_return)\n",
    "\n",
    "        return avg_return\n",
    "\n",
    "    def train_agent(self):\n",
    "        \n",
    "        eval_epochs = []\n",
    "\n",
    "        # Optimize by wrapping some of the code in a graph using TF function.\n",
    "        if self.use_separate_agents:\n",
    "            for car_id in range(self.num_agents):\n",
    "                self.agents[car_id].train = common.function(self.agents[car_id].train)\n",
    "                self.agents[car_id].train_step_counter.assign(0)\n",
    "        else:\n",
    "            self.agent.train = common.function(self.agent.train)\n",
    "\n",
    "        # Compute pre-training returns\n",
    "        if self.use_lstm:\n",
    "            avg_return = self.compute_average_reward_lstm(ego_car_index=0)\n",
    "\n",
    "        else:\n",
    "            avg_return = self.compute_average_reward(ego_car_index=0)\n",
    "\n",
    "        # Log average training return to tensorboard\n",
    "        if self.use_tensorboard:\n",
    "            with self.tb_file_writer.as_default():\n",
    "                tf.summary.scalar(\"Average Eval Reward\", float(avg_return),\n",
    "                                  step=self.global_step)\n",
    "\n",
    "        print(\"DONE WITH PRELIMINARY EVALUATION...\")\n",
    "\n",
    "        # Append for output plot, create video, and empty eval video array\n",
    "        eval_epochs.append(0)\n",
    "        self.create_video(mode='eval', ext=0)\n",
    "        self.video_eval = []  # Empty to create a new eval video\n",
    "        returns = [avg_return]\n",
    "\n",
    "        # Reset the environment time step and global and episode step counters\n",
    "#        time_step = self.train_env.reset()\n",
    "        step = 0\n",
    "        i = 0\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            if self.global_step >= self.total_steps:\n",
    "                print(\"Reached the end of training with {} training steps\".format(self.global_step))\n",
    "                break\n",
    "\n",
    "            ego_car_index = i % self.num_agents\n",
    "            print(\"Training epoch: {}\".format(i))\n",
    "\n",
    "            print(\"Collecting episode for car with ID {}\".format(ego_car_index))\n",
    "\n",
    "            self.video_train = []\n",
    "\n",
    "            if self.use_lstm:\n",
    "                self.collect_episode_lstm(epoch=i, ego_car_index=ego_car_index)\n",
    "                print(\"LSTM\")\n",
    "\n",
    "            else:\n",
    "                self.collect_episode(epoch=i, ego_car_index=ego_car_index)\n",
    "                print(\"No LSTM\")\n",
    "\n",
    "            if i % 100 == 0 and self.add_to_video:\n",
    "                self.create_video(mode='train', ext=i)  \n",
    "            print(\"Collected Episode\")\n",
    "\n",
    "            if self.use_gpu:\n",
    "                device = '/gpu:0'\n",
    "            else:\n",
    "                device = '/cpu:0'\n",
    "\n",
    "            with tf.device(device):\n",
    "\n",
    "                trajectories = self.replay_buffer.gather_all()\n",
    "\n",
    "                if self.use_separate_agents:\n",
    "\n",
    "                    train_loss = self.agents[ego_car_index].train(experience=trajectories)\n",
    "\n",
    "                    if self.use_tensorboard:\n",
    "                        with self.tb_file_writer.as_default():\n",
    "                            tf.summary.scalar(\"Training Loss Agent {}\".format(ego_car_index),\n",
    "                                              float(train_loss.loss),\n",
    "                                              step=self.global_step // self.num_agents)\n",
    "\n",
    "                    step = self.agents[ego_car_index].train_step_counter.numpy()\n",
    "\n",
    "\n",
    "                else:\n",
    "                    train_loss = self.agent.train(experience=trajectories)\n",
    "\n",
    "                    if self.use_tensorboard:\n",
    "                        with self.tb_file_writer.as_default():\n",
    "                            tf.summary.scalar(\"Training Loss\",\n",
    "                                              float(train_loss.loss), step=self.global_step)\n",
    "\n",
    "            with tf.device('/cpu:0'):\n",
    "\n",
    "                if self.global_step % self.log_interval == 0:\n",
    "                    print('step = {0}: loss = {1}'.format(self.global_step,\n",
    "                                                          train_loss.loss))\n",
    "\n",
    "                if i % self.eval_interval == 0:\n",
    "\n",
    "                    if self.use_lstm:\n",
    "                        avg_return = self.compute_average_reward_lstm(ego_car_index=ego_car_index)\n",
    "                    else:\n",
    "                        avg_return = self.compute_average_reward(ego_car_index=ego_car_index)\n",
    "\n",
    "                    if self.use_tensorboard:\n",
    "                        with self.tb_file_writer.as_default():\n",
    "                            tf.summary.scalar(\"Average Eval Reward\",\n",
    "                                              float(avg_return),\n",
    "                                              step=self.global_step)\n",
    "                    eval_epochs.append(i + 1)\n",
    "                    print(\n",
    "                        'epoch = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "                    returns.append(avg_return)\n",
    "                    if self.add_to_video:\n",
    "                        self.create_video(mode='eval', ext=i)\n",
    "                    self.video_eval = []  \n",
    "\n",
    "                if i % self.save_interval == 0 and i != 0:\n",
    "                    self.save_policies(epochs_done=i)\n",
    "                    print(\"Epochs: {}\".format(i))\n",
    "\n",
    "                self.replay_buffer.clear()\n",
    "\n",
    "        if self.use_separate_agents:\n",
    "            return self.agents\n",
    "        else:\n",
    "            return self.agent\n",
    "\n",
    "    def create_video(self, mode='eval', ext=0, ego_car_index=0):\n",
    "        \n",
    "        if mode == 'eval':  \n",
    "            video = self.video_eval\n",
    "        elif mode == 'train':  \n",
    "            video = self.video_train\n",
    "\n",
    "        if len(video) == 0:\n",
    "            raise AssertionError(\"Video is empty.\")\n",
    "        print(\"Number of frames in video: {}\".format(len(video)))\n",
    "        obs_size = video[0].shape\n",
    "        width = np.uint(obs_size[-3])\n",
    "        height = np.uint(obs_size[-2])\n",
    "        channels = np.uint(obs_size[-1])\n",
    "        print(\"HEIGHT IS: {}, WIDTH IS: {}, CHANNELS IS: {}\".format(width, height, channels))\n",
    "\n",
    "        fourcc = cv.VideoWriter_fourcc(*'XVID')\n",
    "        out_file = os.path.join(self.log_dir,\n",
    "                                \"trajectories_{}_epoch_{}_agent_{}\"\n",
    "                                \".avi\".format(mode, ext, ego_car_index))\n",
    "        out = cv.VideoWriter(out_file, fourcc, self.FPS, (width, height))\n",
    "\n",
    "        for i in range(len(video)):\n",
    "            img_rgb = cv.cvtColor(np.uint8(255 * video[i][0]),\n",
    "                                  cv.COLOR_BGR2RGB)  # Save as RGB image\n",
    "            out.write(img_rgb)\n",
    "        out.release()\n",
    "\n",
    "    def plot_eval(self):\n",
    "    \n",
    "        if self.use_separate_agents:  \n",
    "            for car_id in range(self.num_agents):\n",
    "                xs = [i * self.eval_interval for\n",
    "                      i in range(len(self.eval_returns[car_id]))]\n",
    "                plt.plot(xs, self.eval_returns[car_id])\n",
    "                plt.xlabel(\"Training epochs\")\n",
    "                plt.ylabel(\"Average Return\")\n",
    "                plt.title(\"Average Returns as a Function \"\n",
    "                          \"of Training (Agent {})\".format(car_id))\n",
    "                save_path = os.path.join(self.policy_save_dir,\n",
    "                                         \"eval_returns_agent_{}\"\n",
    "                                         \".png\".format(car_id))\n",
    "                plt.savefig(save_path)\n",
    "                print(\"Created plot of returns for agent {}...\".format(car_id))\n",
    "\n",
    "        else:\n",
    "            xs = [i * self.eval_interval for i in range(len(self.eval_returns))]\n",
    "            plt.plot(xs, self.eval_returns)\n",
    "            plt.xlabel(\"Training epochs\")\n",
    "            plt.ylabel(\"Average Return\")\n",
    "            plt.title(\"Average Returns as a Function of Training\")\n",
    "            save_path = os.path.join(self.policy_save_dir, \"eval_returns.png\")\n",
    "            plt.savefig(save_path)\n",
    "            print(\"CREATED PLOT OF RETURNS\")\n",
    "\n",
    "\n",
    "    def save_policies(self, epochs_done=0, is_final=False):\n",
    "       \n",
    "        if is_final:\n",
    "            epochs_done = \"FINAL\"\n",
    "\n",
    "        if self.use_separate_agents:\n",
    "\n",
    "            for i, train_saver in enumerate(self.train_savers):\n",
    "                if custom_path is None:\n",
    "                    train_save_dir = os.path.join(self.policy_save_dir, \"train\",\n",
    "                                                 \"epochs_{}\".format(epochs_done),\n",
    "                                                 \"agent_{}\".format(i))\n",
    "                else:\n",
    "                    train_save_dir = os.path.join(self.policy_save_dir, \"train\",\n",
    "                                                  \"epochs_{}\".format(\n",
    "                                                      custom_path),\n",
    "                                                  \"agent_{}\".format(i))\n",
    "                if not os.path.exists(train_save_dir):\n",
    "                    os.makedirs(train_save_dir, exist_ok=True)\n",
    "                train_saver.save(train_save_dir)\n",
    "\n",
    "            print(\"Training policies saved...\")\n",
    "\n",
    "            for i, eval_saver in enumerate(self.eval_savers):\n",
    "                eval_save_dir = os.path.join(self.policy_save_dir, \"eval\",\n",
    "                                             \"epochs_{}\".format(epochs_done),\n",
    "                                             \"agent_{}\".format(i))\n",
    "                if not os.path.exists(eval_save_dir):\n",
    "                    os.makedirs(eval_save_dir, exist_ok=True)\n",
    "                eval_saver.save(eval_save_dir)\n",
    "\n",
    "            print(\"Eval policies saved...\")\n",
    "\n",
    "        else:\n",
    "            train_save_dir = os.path.join(self.policy_save_dir, \"train\",\n",
    "                                          \"epochs_{}\".format(epochs_done))\n",
    "            if not os.path.exists(train_save_dir):\n",
    "                os.makedirs(train_save_dir, exist_ok=True)\n",
    "            self.train_saver.save(train_save_dir)\n",
    "\n",
    "            print(\"Training policy saved...\")\n",
    "\n",
    "            # Save eval policy\n",
    "            eval_save_dir = os.path.join(self.policy_save_dir, \"eval\",\n",
    "                                         \"epochs_{}\".format(epochs_done))\n",
    "            if not os.path.exists(eval_save_dir):\n",
    "                os.makedirs(eval_save_dir, exist_ok=True)\n",
    "            self.eval_saver.save(eval_save_dir)\n",
    "\n",
    "            print(\"Eval policy saved...\")\n",
    "\n",
    "        agent_params = {'normalize_obs': self.train_env.normalize,\n",
    "                        'use_lstm': self.use_lstm,\n",
    "                        'frame_stack': self.use_multiple_frames,\n",
    "                        'num_frame_stack': self.env.num_frame_stack,\n",
    "                        'obs_size': self.size}\n",
    "\n",
    "        params_path = os.path.join(self.policy_save_dir, \"parameters.pkl\")\n",
    "        with open(params_path, \"w\") as pkl_file:\n",
    "            pickle.dump(agent_params, pkl_file)\n",
    "        pkl_file.close()\n",
    "\n",
    "    def load_saved_policies(self, eval_model_path=None, train_model_path=None):\n",
    "    \n",
    "        if eval_model_path is not None:\n",
    "            self.eval_policy = tf.saved_model.load(eval_model_path)\n",
    "            print(\"Loading evaluation policy from: {}\".format(eval_model_path))\n",
    "\n",
    "        if train_model_path is not None:\n",
    "            self.collect_policy = tf.saved_model.load(train_model_path)\n",
    "            print(\"Loading training policy from: {}\".format(train_model_path))\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"Argument-parsing function for running this code.\"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"-n\", \"--num_agents\", default=2, type=int,\n",
    "                        help=\"Number of cars in the environment.\")\n",
    "    parser.add_argument(\"-size\", \"--size\", required=False,\n",
    "                        default=\"96\",\n",
    "                        help=\"The width and height of the observation window.\")\n",
    "    parser.add_argument(\"-direction\", \"--direction\", type=str, default='CCW',\n",
    "                        help=\"Direction in which agents traverse the track.\")\n",
    "    parser.add_argument(\"-random_direction\", \"--use_random_direction\",\n",
    "                        required=False, action='store_true',\n",
    "                        help=\"Whether agents are trained/evaluated on \"\n",
    "                             \"both CW and CCW trajectories across the track.\")\n",
    "    parser.add_argument(\"-backwards_flag\", \"--backwards_flag\", required=False,\n",
    "                        action=\"store_true\",\n",
    "                        help=\"Whether to render a backwards flag indicator when \"\n",
    "                             \"an agent drives on the track backwards.\")\n",
    "    parser.add_argument(\"-h_ratio\", \"--h_ratio\", type=float, default=0.25,\n",
    "                        help=\"Default height location fraction for where car\"\n",
    "                             \"is located in observation upon rendering.\")\n",
    "    parser.add_argument(\"-ego_color\", \"--use_ego_color\", required=False,\n",
    "                        action=\"store_true\", default=\"Whether to render each \"\n",
    "                                                     \"ego car in the same color.\")\n",
    "\n",
    "    parser.add_argument(\"-self_play\", \"--use_self_play\",\n",
    "                        required=False, action=\"store_true\",\n",
    "                        help=\"Flag for whether to use a single master PPO agent.\")\n",
    "    parser.add_argument(\"-n_agents\", \"--use_separate_agents\",\n",
    "                        required=False, action=\"store_true\",\n",
    "                        help=\"Flag for whether to use a N PPO agents.\")\n",
    "\n",
    "    parser.add_argument(\"-epochs\", \"--total_epochs\", default=1000, type=int,\n",
    "                        help=\"Number of epochs to train agent over.\")\n",
    "    parser.add_argument(\"-steps\", \"--total_steps\", type=int, default=10e6,\n",
    "                        help=\"Total number of training steps to take.\")\n",
    "    parser.add_argument(\"-collect_episode_steps\", \"--collect_steps_per_episode\",\n",
    "                        default=1000, type=int,\n",
    "                        help=\"Number of steps to take per collection episode.\")\n",
    "    parser.add_argument(\"-eval_episode_steps\", \"--eval_steps_per_episode\",\n",
    "                        default=1000, type=int,\n",
    "                        help=\"Number of steps to take per evaluation episode.\")\n",
    "    parser.add_argument(\"-eval_interval\", \"--eval_interval\", default=10,\n",
    "                        type=int,\n",
    "                        help=\"Evaluate every time epoch % eval_interval = 0.\")\n",
    "    parser.add_argument(\"-eval_episodes\", \"--num_eval_episodes\", default=5,\n",
    "                        type=int,\n",
    "                        help=\"Evaluate over eval_episodes evaluation episodes.\")\n",
    "    parser.add_argument(\"-lr\", \"--learning_rate\", default=5e-8, type=float,\n",
    "                        help=\"Learning rate for PPO agent(s).\")\n",
    "    parser.add_argument(\"-lstm\", \"--use_lstm\", required=False, action=\"store_true\",\n",
    "                        help=\"Flag for whether to use LSTMs on actor and critic\"\n",
    "                             \"networks of the PPO agent.\")\n",
    "    parser.add_argument(\"-eps\", \"--epsilon\", type=float, default=0.0,\n",
    "                        help=\"Probability of training on the greedy policy for a\"\n",
    "                             \"given episode\")\n",
    "\n",
    "    parser.add_argument(\"-si\", \"--save_interval\", default=10, type=int,\n",
    "                        help=\"Save policies every time epoch % eval_interval = 0.\")\n",
    "    parser.add_argument(\"-li\", \"--log_interval\", default=1, type=int,\n",
    "                        help=\"Log results every time epoch % eval_interval = 0.\")\n",
    "    parser.add_argument(\"-tb\", \"--use_tensorboard\", required=False,\n",
    "                        action=\"store_true\", help=\"Log with tensorboard as well.\")\n",
    "    parser.add_argument(\"-add_to_video\", \"--add_to_video\", required=False,\n",
    "                        action=\"store_true\",\n",
    "                        help=\"Whether to save trajectories as videos.\")\n",
    "\n",
    "    parser.add_argument(\"-exp_name\", \"--experiment_name\", type=str,\n",
    "                        default=\"experiment_{}\", required=False,\n",
    "                        help=\"Name of experiment (for logging).\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(\"Your selected training parameters: \\n {}\".format(vars(args)))\n",
    "    return args\n",
    "\n",
    "\n",
    "def main():\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08e3f20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
